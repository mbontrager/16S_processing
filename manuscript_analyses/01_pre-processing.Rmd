---
output: html_document
---
#E affinis 16S analysis
### Martin Bontrager
### Data pre-processing

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs_preprocess/',
                      echo=FALSE, warning=FALSE, message=FALSE)
```

# Sample Summary

First I will read in all of the raw count data, the phylogenetic tree, and the taxonomy. These data were all generated from the "UPARSE_pipeline.py" script.

## Sample information and summary statistics:

```{r read_data}
library(phyloseq)
library(ape)
library(tidyr)
library(dplyr)
library(reshape2)
library(knitr)

set.seed(9373399)
#Change the following to true if you need a .csv output table.
output_table = TRUE

## Read in the input table, taxonomy
otuTable <- read.table("data/otutable.txt", header = TRUE, row.names = 1)
rownames(otuTable) <- lapply(rownames(otuTable), sub, pattern = "(^OTU_.*);.*;",
                             replacement = "\\1")
taxTable <- read.table("data/otu.uchime.gg_13_8_FWSET.wang.taxonomy", sep = '')

# Clean up taxonomy table labels and columns (remove chars from greengenes)
# and fix OTU labels so they don't include `size=`
taxTable$V1 <- lapply(taxTable$V1, sub, pattern = "(^OTU_.*);.*;", 
                      replacement = "\\1")
taxTable$V2 <- as.character(taxTable$V2)
colNames <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", 
              "Species", "DELETE", "DELETE2")
taxTable <- separate(taxTable, V2, into = colNames, ";")
taxTable$DELETE <- NULL; taxTable$DELETE2 <- NULL
colnames(taxTable)[1] <- "OTU"
removeWords <- c("k__", "p__", "c__", "o__", "f__", "g__", "s__", "\\(..\\)", 
                 "\\(...\\)", "\\]", "\\[")
taxTable <- (as.data.frame(sapply(taxTable, function(x)
        gsub(paste(removeWords, collapse="|"), "", x))))
taxmat <- as.matrix(taxTable)
rownames(taxmat) <- taxmat[, 1]
taxmat <- taxmat[, -1]

# Create Phyloseq object
OTU <- otu_table(otuTable, taxa_are_rows = TRUE)
TAX <- tax_table(taxmat)
physeq <- phyloseq(OTU, TAX)

#Merge techincal replicates into samples
replicates <- read.csv("data/replicates.csv", header = TRUE, row.names = 1)
sample_data(physeq) <- sample_data(replicates)
physeq <- merge_samples(physeq, "ID")
sampleData <- read.csv("data/16S_metadata.csv", header = TRUE, row.names = 1)
sample_data(physeq) <- sample_data(sampleData)
sample_data(physeq)$HostxWater <- paste(sample_data(physeq)$Environment, 
                                        sample_data(physeq)$WaterType)
sample_data(physeq)$CladexWater <- paste(sample_data(physeq)$Clade, 
                                          sample_data(physeq)$WaterType)
sample_data(physeq)$LatitudexWater <- paste(sample_data(physeq)$Latitude, 
                                          sample_data(physeq)$WaterType)
sample_data(physeq)$CladexHost <- paste(sample_data(physeq)$Environment, 
                                          sample_data(physeq)$Clade)

# Read tree file
fasttree <- read.tree("data/ParsedFastTree2.tre")
physeq <- merge_phyloseq(physeq, fasttree)
rm(OTU, TAX, fasttree, taxTable, taxmat, sampleData, removeWords, otuTable, 
   replicates)

# Remove samples with low read counts
physeq <- prune_samples(sample_sums(physeq)>=500, physeq)

# Remove halomonas,shewanella, and propionobacterium contaminants
# We've identified these as contaminants based on negative control samples
# and their absence from our first round of sequencing. 
contaminants <- !(taxa_names(physeq) %in% c("OTU_1", "OTU_5", "OTU_45"))
physeq <- prune_taxa(contaminants, physeq)

# Remove samples because they are either old (RAE), highly contaminated(MIE, TBE), redundant (V2E) or inappropriate (V1W due to use of the 0.8 micron filter)
a <- sample_names(physeq) %in% c("AE", "V1W", "V1Wp", "V2E", "VGUN", "LOGUN", "VG", "VE", "V1Ep", "RE", "CE", "BBE1", "MIE", "MIW", "TBE", "TBW", "BBE2", "POE1", "POE2", "RAE", "RAW", "SJE")
physeq <- prune_samples(!a, physeq)

# Remove OTUs with zero representatives now that samples have been trimmed
physeq <- prune_taxa(taxa_sums(physeq) > 1, physeq)

# Export csv count table if necessary (for the sake of exploration)
# I either normalize everything percent of total per sample or just 
# leave the raw count data depending on if line 1 or 2 of the if 
# statement is commented out. Better normalization with variance stabilization
# is performed during beta diversity analyses

if (output_table){
    #physeq1 <- transform_sample_counts(physeq, function(x) round(100 * (x/sum(x)), 2))
    physeq1 <- physeq
    b <- psmelt(physeq1)[, 1:3]
    c <- dcast(b, OTU ~ Sample)
    e <- as.data.frame(tax_table(physeq))
    e <- cbind(OTU = rownames(e), e)
    f <- merge(e, c, by = "OTU")
    d <- sapply(f[,1], function(x) gsub("OTU_", "", x), USE.NAMES = FALSE)
    d <- as.numeric(d)
    f$OTU <- sprintf("OTU_%05d", d)
    f <- f[order(f$OTU),]
    write.csv(f, "output/sample_matrix_raw_counts.csv")
    rm(physeq1, b, c, d, e, f)
}

# Print a table of read counts for each sample
read_count <- as.data.frame(sample_sums(physeq))
read_count <- cbind(Sample=rownames(read_count), read_count)
sdata <- sample_data(physeq)
sdata <- cbind(Sample=rownames(sdata), sdata)
metadata <- merge(read_count, sdata, by = "Sample")[,1:11]
colnames(metadata)[2] <- "ReadCount"
kable(metadata)
```

## Read count information:

``` {r min_med_max}
# Print mean and median sample read counts
print(summary(sample_sums(physeq)))
```

```{r finalize}
# Output phyloseq data for subsequent analysis
otus <- otu_table(physeq)
write.csv(otus, file='data/otus.csv')
taxa <- tax_table(physeq)
write.csv(taxa, file='data/taxa.csv')
sample <- sample_data(physeq)
write.csv(sample, 'data/sampleData.csv')
tree <- phy_tree(physeq)
write.tree(tree, file="data/tree.tre")
```
